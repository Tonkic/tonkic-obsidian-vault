ResNet 的设计就像 **学新知识时参考旧笔记**：
- **残差块（核心组件）**：
    - **原有路径**：输入 → 卷积层1 → 卷积层2 → 输出
    - **跳跃连接**：把输入直接抄一份，绕过卷积层，直接加到输出上
    - **最终输出** = 卷积层结果 + 原始输入
**类比**：  
假设你已经会计算 `5+3=8`，现在要学 `5+3+2=10`：
- 普通网络：重新学 `5+3+2`
- ResNet：直接学 `8+2`（利用已有的 `5+3=8` 结果）
  
效果：
防退化：即使新增的层没学到东西，输出也不会变差（输出 = 原有输入 + 0）。
梯度直达：反向传播时，梯度可通过跳跃连接直接传回浅层，避免中途消失。
增量学习：每个残差块只需学习「新差异」，无需重复已掌握的知识。