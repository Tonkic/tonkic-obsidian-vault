Google AI 在2018年提出的一种基于[[Transformer]] 架构的双向预训练语言处理模型

BERT 的输入是文本序列的向量模式，包括词向量、文本向量和位置向量
词向量 一个词对于什么意思的字典
文本向量 一整个句子的意思
位置向量 句子中词的顺序

在预训练的时候，使用【mask】遮挡某些字词
NSP任务 预测给定的两个句子是否相邻