---
tags:
  - 机器学习
---
![../../../pic/Pasted image 20250224040659.png](../../../pic/Pasted%20image%2020250224040659.png)
# 以中英文翻译为例
## embedding-解决单个词的语义问题
编写词典，在训练的时候回不断的改变上图红框内矩阵的参数
比如用\[1.0,0.2,0.5,0.4\]表示苹果，向量的每个维度可能是一个抽象的语义，比如苹果的1.0可能是水果的意思，语义相近的词在潜空间里向量也邻近
## 注意力机制-解决词在上下文中的含义
![../../../pic/Pasted image 20250224041122.png](../../../pic/Pasted%20image%2020250224041122.png)
以我看图为例，我就是Q，图就是V，我需要在图中注意到信息量高的信息（宝宝的人脸，文章的标题）
以中英翻译为例，这里的输入就是T个词语，$D_{in}$就是词向量的维度
Q乘以$K^{T}$(其实就是矩阵内向量内积，表示余弦相似度)得到T\*T的矩阵A称为相似度（需要除以$\sqrt{D_{\text{out}}}$让数值分散）
Q乘T转置的这一步，如果向量服从标准正太分布，原算完得到的A~N（0，$D_{\text{out}} * I$）I为单位阵
A’表示上下文对这个词向量的影响系数，A中的一个数字就是两个向量点积，可以理解成两个向量的余弦相似度，A整个矩阵就是这个向量和包括自己在内的所有上下文向量的余弦相似度集合
## positonal encodeing-解决一个词在语句中位置关系
没看懂，大概是用矩阵加法，通过傅里叶函数理解，体现词向量在句子中的位置关系（你打我和我打你字一样，位置关系不同意思完全不一样）